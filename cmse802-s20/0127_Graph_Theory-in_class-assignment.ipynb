{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to participate both individually and in groups during class on **Monday January 27**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# In-Class Assignment: Graph Theory\n",
    "<a href=\"http://sixdegrees.hu/last.fm/\"><img src=\"http://sixdegrees.hu/last.fm/images/lastfm_360_graph_white.png\"><p style=\"text-align: right;\">\n",
    "network of musicians\n",
    "</p>\n",
    "<p style=\"text-align: right;\">Interactive Image</p></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Agenda for today's class (80 minutes)\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "1. [(20 minutes) Review Code Optimization](#Review_Code_Optimizaiton)\n",
    "1. [(20 minutes) Optimization Methods Report Review](#Optimization_Methods_Report_Review)\n",
    "1. [(20 minutes) Review of pre-class assignment](#Review_of_pre-class_assignment)\n",
    "1. [(20 minutes) Web Scraping](#Web_Scraping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Review_Code_Optimizaiton\"></a>\n",
    "\n",
    "# 1. Review Code Optimization\n",
    "\n",
    "- [0122-Code_Optimization_in-class-assignment](0122-Code_Optimization_in-class-assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Optimization_Methods_Report_Review\"></a>\n",
    "# 2. Optimization Methods Report Review\n",
    "\n",
    "Break up into groups (find people you don't know) and discuss your Optimization reports from Friday. Share a summary of what you learned and some of the ideas.  Again, the goal is to learn something from each other.  \n",
    "\n",
    "Be prepared to share interesting things from your discussion with the entire class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Review_of_pre-class_assignment\"></a>\n",
    "# 3. Review of pre-class assignment\n",
    "\n",
    "* [0126-Graph_Theory-pre-class-assignment](0126-Graph_Theory-pre-class-assignment.ipynb)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Web_Scraping\"></a>\n",
    "# 4. Web Scraping\n",
    "As a group run and review the code and answer the questions below.\n",
    "\n",
    "Note: example modified from http://www.netinstructions.com/how-to-make-a-web-crawler-in-under-50-lines-of-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from html.parser import HTMLParser  \n",
    "from urllib import parse\n",
    "from urllib.request import urlopen  \n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a class called LinkParser that inherits some\n",
    "# methods from HTMLParser which is why it is passed into the definition\n",
    "class LinkParser(HTMLParser):\n",
    "\n",
    "    # This is a function that HTMLParser normally has\n",
    "    # but we are adding some functionality to it\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # We are looking for the begining of a link. Links normally look\n",
    "        # like <a href=\"www.someurl.com\"></a>\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    # We are grabbing the new URL. We are also adding the\n",
    "                    # base URL to it. For example:\n",
    "                    # www.netinstructions.com is the base and\n",
    "                    # somepage.html is the new URL (a relative URL)\n",
    "                    #\n",
    "                    # We combine a relative URL with the base URL to create\n",
    "                    # an absolute URL like:\n",
    "                    # www.netinstructions.com/somepage.html\n",
    "                    newUrl = parse.urljoin(self.baseUrl, value)\n",
    "                    # And add it to our colection of links:\n",
    "                    self.links = self.links + [newUrl]\n",
    "\n",
    "    # This is a new function that we are creating to get links\n",
    "    # that our spider() function will call\n",
    "    def getLinks(self, url):\n",
    "        self.links = []\n",
    "        # Remember the base URL which will be important when creating\n",
    "        # absolute URLs\n",
    "        self.baseUrl = url\n",
    "        # Use the urlopen function from the standard Python 3 library\n",
    "        response = urlopen(url)\n",
    "        # Make sure that we are looking at HTML and not other things that\n",
    "        # are floating around on the internet (such as\n",
    "        # JavaScript files, CSS, or .PDFs for example)\n",
    "        if 'text/html' in response.getheader('Content-Type'):\n",
    "            htmlBytes = response.read()\n",
    "            # Note that feed() handles Strings well, but not bytes\n",
    "            # (A change from Python 2.x to Python 3.x)\n",
    "            htmlString = htmlBytes.decode(\"utf-8\")\n",
    "            self.feed(htmlString)\n",
    "            return htmlString,self.links #htmlString, self.links\n",
    "        if 'text/plain' in response.getheader('Content-Type'):\n",
    "            return url,[]\n",
    "        else:\n",
    "            return \"\",[]\n",
    "\n",
    "# And finally here is our spider. It takes in an URL, a word to find,\n",
    "# and the number of pages to search through before giving up\n",
    "def spider(url, maxPages):  \n",
    "    pagesToVisit = [url]\n",
    "    numberVisited = 0\n",
    "    foundWord = False\n",
    "\n",
    "    # The main loop. Create a LinkParser and get all the links on the page.\n",
    "    # Also search the page for the word or string\n",
    "    # In our getLinks function we return the web page\n",
    "    # (this is useful for searching for the word)\n",
    "    # and we return a set of links from that web page\n",
    "    # (this is useful for where to go next)\n",
    "    while numberVisited < maxPages and pagesToVisit != []:\n",
    "        numberVisited = numberVisited +1\n",
    "        # Start from the beginning of our collection of pages to visit:\n",
    "        url = pagesToVisit[0]\n",
    "        pagesToVisit = pagesToVisit[1:]\n",
    "\n",
    "        try:\n",
    "            print(numberVisited, \"Visiting:\", url)\n",
    "            parser = LinkParser()\n",
    "            data, links = parser.getLinks(url)\n",
    "            pagesToVisit = pagesToVisit + links\n",
    "        except:\n",
    "            print(\" **Failed to parse page!**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = spider('http://www.msu.edu/', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "We would like to modify the above code to generate a graph of websites visited by the web scraper.  Nodes are the website URLs and the edges are the links between the websites. We would then like to plot the adjacency list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font>   things you would like to see in a programming module for graphs?  In other words, what kinds of things would you want to see in a graph theory library to help solve the problem above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your selection criteria here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> - Share your ideas with your group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write any additional selection criteria that came up during your group discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> - Select someone from your group to share your group's ideas with the class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write any additional selection criteria that came up during the class discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Everyone get out their laptops and search for a programming library that will support graph theory.  Pick your favorite based on the properties and features you wanted during group discussion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list here of the libraries you found. mark the one that you feel best matches your criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Modify the above code to generate a graph of of links using the LinkParser from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> As a group modify the above code to generate an adjacency list of websites visited by the web scraper.  Nodes are the website URLs and the edges are the links between the websites. Plot the resulting graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    " Now that we have some idea about how the above code works. Think about what you would need to do to download sub-hourly temperature data for Michigan from the NOAA climate reference network:\n",
    "\n",
    "https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/us-climate-reference-network-uscrn\n",
    "\n",
    "&#9989; <font color=red>**DO THIS:**</font> In your group, see if you can figure out how to adjust the above code to scrape the aboe website and download all the temperature data for a particular observatory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Resources:\n",
    "\n",
    "- [Syllabus](https://docs.google.com/document/d/e/2PACX-1vTW4OzeUNhsuG_zvh06MT4r1tguxLFXGFCiMVN49XJJRYfekb7E6LyfGLP5tyLcHqcUNJjH2Vk-Isd8/pub)\n",
    "- [Preliminary Schedule](https://docs.google.com/spreadsheets/d/e/2PACX-1vRsQcyH1nlbSD4x7zvHWAbAcLrGWRo_RqeFyt2loQPgt3MxirrI5ADVFW9IoeLGSBSu_Uo6e8BE4IQc/pubhtml?gid=2142090757&single=true)\n",
    "- [D2L Page](https://d2l.msu.edu/d2l/home/912152)\n",
    "- [Git Repository](https://gitlab.msu.edu/colbrydi/cmse802-s20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2020,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
