{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to participate both individually and in groups during class on **Wednesday January 29**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment: Instructor template\n",
    "\n",
    "We will be using some of the extensive datasets available at the National Oceanic and Atmospheric Administration (NOAA). \n",
    "\n",
    "<a href=\"http://www.noaa.gov/\"><img width=200 align='center' src=\"http://www.nssl.noaa.gov/projects/debrisflow09/NOAA%20Circle.gif\"></a>\n",
    "\n",
    "Image From: https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/us-climate-reference-network-uscrn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda for today's class (80 minutes)\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. [(20 minutes) Review pre-class assignment](#Review_pre-class_assignment)\n",
    "2. [(25 minutes) NOAA Example](#NOAA_Example)\n",
    "1. [(10 minutes) Installing Beautiful Soup](#Installing_Beautiful_Soup)\n",
    "2. [(25 minutes) Presidential data example](#Presidential_data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Review_pre-class_assignment\"></a>\n",
    "\n",
    "# 1. Review pre-class Assignment\n",
    "\n",
    "- [0128--Web_Scraping-pre-class-assignment](0128--Web_Scraping-pre-class-assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"NOAA_Example\"></a>\n",
    "\n",
    "# 2. NOAA Example and Coding Standards.\n",
    "\n",
    "In the course git repository there is now a ```noaa_scrapper.py``` file.  Load the file via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from noaa_scraper import get_noaa_temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Please run the ```get_noaa_temperatures``` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_temperatures = get_noaa_temperatures('http://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/', 'Gaylord', 100)\n",
    "plt.plot(air_temperatures)\n",
    "# plt.axis([0,1000,-20,80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> With your group, do a code review of the contents of the **noaa_scraper.py** file and figure out what it does. What are the main part of this module and what do they do? Be prepared to discuss this with the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your notes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Let's look more closely at the code.  Run the folloing ```pylint``` command and figure out what the output is telling you (Hint: use the Internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pylint noaa_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Install and run ```autopep8``` library. There are two options depending on your setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ```autopep8```globally or in an environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autopep8 noaa_scraper.py > noaa_scraper2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install ```autopep8``` in a local packages folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -t packages autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this if autopep8 was installed locally\n",
    "#python ./packages/autopep8.py noaa_scraper.py  > noaa_scraper2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pylint noaa_scraper2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> In your group discuss the ```pylint``` and ```autopep8``` commands.  We will be using these commands in an upcoming Project assignment. What do they do? Is this useful or does it just add more work?  Be prepared to discuss your answers with the rest of class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your discussion notes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Installing_Beautiful_Soup\"></a>\n",
    "\n",
    "# 3. Installing Beautiful Soup\n",
    "\n",
    "For this class we will be trying out [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python web parsing module. \n",
    "\n",
    "&#9989; <font color=red>**DO THIS:**</font> Install the ```beautifulsoup4``` library on your computer (the following will work on jupyterhub but should work anywhere).  When you are done, help your neighbor and raise your hand if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -t ./packages/ beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./packages/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<a name=\"Presidential_data_example\"></a>\n",
    "# 4. Presidential data example\n",
    "\n",
    "Found this idea by reading the following blog post: https://blog.exploratory.io/scraping-us-presidents-list-from-web-and-transforming-it-to-be-useful-fff534470bb6\n",
    "\n",
    "&#9989; <font color=red>**DO THIS:**</font> Click on the following link and review the page source with your neighbor.  Discuss which tags you need to look for to try and isolate the table data only.  Ideally we want to create a ```pandas table``` of this data:\n",
    "https://www.loc.gov/rr/print/list/057_chron.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put notes on what you find here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and view html\n",
    "\n",
    "The following code should download the above website and parse read it into a ```beautifulsoup``` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following library downloads the data and stores it in a page variable\n",
    "import requests\n",
    "page = requests.get(\"https://www.loc.gov/rr/print/list/057_chron.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and run beautifule should html.parser\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> explore the ```soup``` variable using python functions such as; ```type```, ```dir``` and ```help```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your answer to the above here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the raw html using \"pretty print\" \n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Tables\n",
    "\n",
    "Next, the following code finds all of the ```table``` sections in the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above the results show that there are 9 ```table``` objects in the document.  We are just looking for the one that has our data in it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; <font color=red>**DO THIS:**</font> Find the table from the nine tables that has only the data we want. Make a variable ```table``` that only includes the information we want. Hint, it is not the first table which we can see by using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[0]\n",
    "print(table.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse out all the rows\n",
    "\n",
    "The rows of a table are determined by the ```tr``` (table row) tag and the columns are determined by the ```td```. The following code can find all of the rows in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the column labels\n",
    "\n",
    "The first row is the column header row as can be seen by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for c in rows[0].find_all('th'):\n",
    "    labels.append(c.get_text())\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Rows\n",
    "\n",
    "&#9989; <font color=red>**DO THIS:**</font> The next step is to loop though the remaining rows and save the data as a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert list of list to Pandas Dataframe\n",
    "\n",
    "Assuming the above works, we can convert the list of lists and labels to a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "    \n",
    "# Create the pandas DataFrame  \n",
    "df = pd.DataFrame(data, columns=labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Resources:\n",
    "\n",
    "- [Syllabus](https://docs.google.com/document/d/e/2PACX-1vTW4OzeUNhsuG_zvh06MT4r1tguxLFXGFCiMVN49XJJRYfekb7E6LyfGLP5tyLcHqcUNJjH2Vk-Isd8/pub)\n",
    "- [Preliminary Schedule](https://docs.google.com/spreadsheets/d/e/2PACX-1vRsQcyH1nlbSD4x7zvHWAbAcLrGWRo_RqeFyt2loQPgt3MxirrI5ADVFW9IoeLGSBSu_Uo6e8BE4IQc/pubhtml?gid=2142090757&single=true)\n",
    "- [D2L Page](https://d2l.msu.edu/d2l/home/912152)\n",
    "- [Git Repository](https://gitlab.msu.edu/colbrydi/cmse802-s20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2020,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
