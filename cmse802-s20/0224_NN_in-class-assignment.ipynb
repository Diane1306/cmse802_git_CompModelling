{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully complete this assignment you need to participate both individually and in groups during class on **Monday February 24**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment: Training an Artificial Neural Network\n",
    "\n",
    "</p>\n",
    "\n",
    "<img src= \"https://ml4a.github.io/images/temp_fig_mnist.png\">\n",
    "<p style=\"text-align: right;\">From: Machine Learning for Artists - https://ml4a.github.io/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda for today's class (80 minutes)\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "1. [(20 minutes) ABM Report review](#ABM_Report_review)\n",
    "1. [(20 minutes) Review pre-class assignment](#Review_pre-class_assignment)\n",
    "1. [(10 minutes) The universal approximation theorem](#The_Universal_Approximation_Theorem)\n",
    "1. [(20 minutes) Guessing a Neural Network Model](#Guessing_a_Neural_Network_Model)\n",
    "1. [(10 minutes) Mean Squared Error](#T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"ABM_Report_review\"></a>\n",
    "# 1. ABM Report review\n",
    "\n",
    "Discuss what you learned about how ABMs relate to your research with your neighbors. Think about commonalities between your projects.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Review_pre-class_assignment\"></a>\n",
    "# 2. Review pre-class assignment:\n",
    "\n",
    "* [0223-NN--pre-class-assignment](0223-NN--pre-class-assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"The_Universal_Approximation_Theorem\"></a>\n",
    "# 3. The Universal Approximation Theorem\n",
    "\n",
    "As a group, please discuss how (if at all) the theorem below relates to the topic at hand.\n",
    "\n",
    "\n",
    "> In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of $R^n$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.\n",
    "\n",
    ">One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid activation functions.\n",
    "\n",
    ">Kurt Hornik showed in 1991 that it is not the specific choice of the activation function, but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators.\n",
    "<p style=\"text-align: right;\">From: Wikipidia - https://en.wikipedia.org/wiki/Universal_approximation_theorem</p>\n",
    "\n",
    "### Some Math\n",
    "Let $\\varphi(\\cdot)$ be a nonconstant, bounded, and monotonically-increasing continuous function. Let $I_m$ denote the $m$-dimensional unit hypercube $[0,1]^m$. The space of continuous functions on $I_m$ is denoted by $C(I_m)$. Then, given any function $f\\in C(I_m)$ and $\\epsilon>0$, there exists an integer $N$, real constants $v_i, b_i \\in \\mathbb{R}$ and real vectors $\\mathbf{w}_i \\in \\mathbb{R}^m$, where $i = 1, \\ldots, N$ such that if we define: \n",
    "$$F(\\mathbf{x}) = \\sum_{i=1}^N v_i \\cdot\\varphi \\big(\\langle\\mathbf{w}_i , \\mathbf{x}\\rangle + b_i\\big) $$\n",
    "then \n",
    "$$|F(\\mathbf{x}) - f(\\mathbf{x})| < \\epsilon $$\n",
    "\n",
    "for all $x\\in I_m$. In other words, functions of the form $F(\\mathbf{x})$ are dense in $C(I_m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>** In simplest terms, why do we care about the Universal Approximation Theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer to the above question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Guessing_a_Neural_Network_Model\"></a>\n",
    "# 4. Guessing a Neural Network Model\n",
    "Imagine we want to train a deep neural network with several hidden layers. So, we have a big array of weights $W$ (for all the synapses and all the layers), several biases $b$ and training data $\\big\\{(x_l , y_l) :  l=1,\\ldots, L \\big\\}$. For each choice of weights and biases,  forward propagation produces an estimate $\\hat{y}(x_l)$ of $y_l$ for each training data point $x_l$. (This is what we did in the pre-class assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we solve the ANN problem lets use a simpler model:\n",
    "\n",
    "Suppose we have the experimental data (```x_data``` and ```y_data```) given below, and that we want to fit a linear model $y = mx + b$ to this data.  How to we find $m$ and $b$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>** -  First, lets just try to guess different values for $m$ and $b$ that best fit the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## experimental data\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "x_data = np.array([1, 2, 2.5, 3, 3.5, 4.5, 4.7, 5.2, 6.1, 6.1, 6.8])\n",
    "y_data = np.array([1.5, 1, 2, 2, 3.7, 3, 5, 4, 5.8, 5, 5.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = -0.5\n",
    "b = 2\n",
    "\n",
    "plt.scatter(x_data,y_data)\n",
    "x_range = np.array([i for i in range(0, 8)])\n",
    "y_range = m*x_range + b #Equation of a line\n",
    "plt.plot(x_range,y_range)\n",
    "len(x_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>QUESTION:</font>** What values did you find for $m$ and $b$ and how do you know your selection of $b$ and $m$ are the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Mean_Squared_Error\"></a>\n",
    "# 5.  Mean Squared Error\n",
    "\n",
    "We can use the Mean Squared Error (MSE) to measure how good our choice is for $b$ and $m$. The general $MSE$ equation is as follows: \n",
    "$$MSE(W,b)  = \\frac{1}{L} \\sum_{i=1}^L (y_i - \\hat{y}(x_l))^2$$\n",
    "\n",
    "If we plug in our liniar funciton $\\hat{y} = m\\cdot x + b$ we get:\n",
    "\n",
    "$$MSE(W,b)  = \\frac{1}{L} \\sum_{i=1}^L ( y_i - (m\\cdot x_i + b))^2$$\n",
    "\n",
    "One strategy is to find values of $w$ and $b$ which minimize this expression. Keep in mind that $MSE$ is a function of more than one variable, with a sum over potentially many training data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>DO THIS:</font>**  Write a function (called MSE) that takes the guesses for $b$, $m$ and the experimental data ```x_data``` and ```y_data``` as inputs and returns the $MSE$ for the function $y = m\\cdot x + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure your funciton works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = MSE(b, m, x_data, y_data)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Resources:\n",
    "\n",
    "- [Syllabus](https://docs.google.com/document/d/e/2PACX-1vTW4OzeUNhsuG_zvh06MT4r1tguxLFXGFCiMVN49XJJRYfekb7E6LyfGLP5tyLcHqcUNJjH2Vk-Isd8/pub)\n",
    "- [Preliminary Schedule](https://docs.google.com/spreadsheets/d/e/2PACX-1vRsQcyH1nlbSD4x7zvHWAbAcLrGWRo_RqeFyt2loQPgt3MxirrI5ADVFW9IoeLGSBSu_Uo6e8BE4IQc/pubhtml?gid=2142090757&single=true)\n",
    "- [D2L Page](https://d2l.msu.edu/d2l/home/912152)\n",
    "- [Git Repository](https://gitlab.msu.edu/colbrydi/cmse802-s20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2020,  Michigan State University Board of Trustees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
